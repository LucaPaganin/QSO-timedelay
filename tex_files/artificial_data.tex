\section{Structure function method}

\subsection{Structure function estimation from data}

Like in \cite{Press1992} one has to estimate from the data the so-called structure function

\begin{equation}
V(\tau) \equiv \frac{1}{2} \left \langle \left [ s(t+\tau) - s(t) \right ]^2  \right \rangle
\end{equation}

This is related to the covariance (or autocorrelation function) of the time serie (see \cite{Press1992}) for all details)

\begin{equation}
C(\tau) = \langle s(t)^2 \rangle - V(\tau)
\end{equation}

\subsection{Generating light curves from data}

\subsubsection{Eigenvalue decomposition}

Consider a given collection of $N$ observation times $t_1, \dots , t_N$. Construct the augmented observation time list appending to the end a the collection of the delayed observation times:

\[
t_1, \dots , t_N, t_1 - \Delta t, \dots, t_N - \Delta t
\]

with an assumed known delay $\Delta t$. After that use the estimated structure function to build a $2N \times 2N$ covariance matrix

\begin{equation}
C_{ij} = \langle s(t)^2 \rangle - V(\tau_{ij})
\end{equation}

with $\tau_{ij}$ being the lag matrix defined by

\begin{equation}
\tau_{ij} = |t_i - t_j|, \quad i,j = 1, \dots , 2N
\end{equation}

with the relation

\begin{equation}
t_{i+N} = t_i - \Delta t
\end{equation}

What they say in \cite{Press1992} is to perform the eigenvalue decomposition of the covariance matrix $C$

\begin{equation}\label{eq:eigen-cov-decomp}
C = Z^\dagger \Lambda Z
\end{equation}

with $\Lambda$ being the diagonal matrix with the eigenvalues

\[
\Lambda = \mathrm{diag} \left ( \lambda_1, \dots, \lambda_{2N} \right )
\]
 
 and $Z$ the orthogonal matrix which diagonalize $C$, the rows of $Z$ being the eigenvectors of $C$. After that one can generate a dataset with the following prescription
 
\begin{equation}
y_i = \sum_{i=1}^{2N} Z_{ij} \sqrt{\lambda_j} r_j + \langle n_i^2 \rangle^{1/2} r_i
\end{equation}

with $ \langle n_i^2 \rangle = e_i^2$ the squares of the errors on the data, and $r_i$, $r_j$ are independent uncorrelated gaussian random numbers with zero mean and unit variance. However we have tried this method on real data, and what it happens is that is not numerically stable. The eigenvalues span many orders of magnitude and this causes data with very high unrealistic peaks.

\subsubsection{Cholesky and Eigenvalue decompositions}

Assuming that $C$ is a real symmetric and positive definite matrix, it can be Cholesky decomposed

\begin{equation}
C = L L^{\dagger}
\end{equation}

with $L$ being a lower triangular matrix called the \emph{Cholesky factor} of $C$. We can show that there is a relation between the Cholesky and eigenvalue factorizations of a given matrix. First observe that, from the eigenvalue decomposition:

\begin{align*}
C = Z^\dagger \Lambda Z = Z^\dagger \sqrt{\Lambda} \sqrt{\Lambda} Z = \left ( \sqrt{\Lambda} Z \right )^{\dagger} \sqrt{\Lambda} Z
\end{align*}

with $\sqrt{\Lambda}$ being a diagonal matrix that multiplied by itself gives $\Lambda$, i.e. a diagonal matrix with the square roots of the eigenvalues as its entries. Now remember that any real square matrix can be factorized into a unitary matrix $Q$ and an upper triangular matrix $R$, the so-called $QR$-factorization. Doing this on the matrix $\sqrt{\Lambda} Z$ we have

\begin{equation}
\sqrt{\Lambda} Z = Q R
\end{equation}

But now we can rewrite the \eqref{eq:eigen-cov-decomp} as

\[
C = (QR)^{\dagger} QR = R^{\dagger} Q^{\dagger} Q R = R^{\dagger} R 
\]

being $Q$ unitary. Finally, since $R$ is upper triangular, it conjugate transposed 

\begin{align*}
L = R^{\dagger}
\end{align*}
 
is lower triangular. Therefore we have obtained the CHolesky decompotision of $C$

\begin{equation}
 C = L L^{\dagger}
\end{equation} 

with

\[
L = \left ( \sqrt{\Lambda} Z \right )^{\dagger} Q
\]

\subsubsection{Reformulating the prescription}

Using the results of the previous section we can rewrite the prescription for the data generation (leave outside the noise for now)

\begin{equation}
\bv{y} = \left ( Z \sqrt{\Lambda} \right ) \bv{r} 
\end{equation}

Instead of using $Z$ use $Z^{\dagger}$ and obtain 

\begin{align*}
Z^{\dagger} \sqrt{\Lambda} = \left ( \sqrt{\Lambda} Z\right )^{\dagger} = L Q^{\dagger}
\end{align*}

The we can substitute the eigenvalue decomposition with the Cholesky one, which is numerically more stable. For simplicity we simply write

\begin{equation}
\bv{y} = L \bv{r}
\end{equation}

For what regards the noise, in ref \cite{Press1992} they identify the expectation value $\langle n_i^2 \rangle$ with the square of the quoted error on the magnitude. This makes sense if one assumes that $n_i$ is a random gaussian number with zero mean and variance given by the experimental error. So

\begin{equation}
\langle n_i^2 \rangle = e_i^2
\end{equation}

Then our prescription to generate the dataset becomes

\begin{align}\label{eq:reform-artificial-prescr}
y_i = \sum_{j=1}^{2N} L_{ij} r_j + e_i r_i, \quad i = 1, \dots , 2N
\end{align}

where $L$ is the lower triangular Cholesky factor of the estimated covariance $C$, i.e.

\[
C = L L^{\dagger}
\]

We interpret the first $N$ points $y_1, \dots, y_N$ as measurement of a mock light curve at times $t_1, \dots , t_N$, 
while the other $N$ points $y_{N+1}, \dots, y_{2N}$ are measurements of the \emph{same} mock curve (apart from noise) at times 
$t_1-\Delta t, \dots , t_N-\Delta t$, where the true delay can be decided \emph{a priori} when building the covariance. Therefore, if we assign each point of the second curve at times $t_1, \dots , t_N$ it will exactly experience a delay $\Delta t$.

In figure \ref{fig:artif-dataset-5-vs-real} we see a mock light curve compared to the true light curve from which it has been generated. Both the curves have been scaled to have zero mean and unit variance, in order to make a direct comparison. In figure \ref{fig:artif-dataset-5-shift-comparison} we can see a comparison between the mock curves of the same mock dataset, with a relative time delay of about $18$ days.

\begin{figure}
\centering
\includegraphics[scale=0.45]{realiz5_comparison.pdf}
\caption{Realization 5 vs true quasar data, image A.}
\label{fig:artif-dataset-5-vs-real}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.45]{Montecarlo_y_vs_y_delayed.pdf}
\caption{Realization 5: mock light curve vs its shifted version, with a true delay of about $18$ days.}
\label{fig:artif-dataset-5-shift-comparison}
\end{figure}


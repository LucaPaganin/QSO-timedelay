{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, minmax_scale\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from scipy import stats\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reading and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(r'C:\\Users\\lbg\\Desktop\\HE0435_NMC_1e5_curvesum.h5', 'r')\n",
    "X =f['X']\n",
    "Y =f['y']\n",
    "scale = True\n",
    "\n",
    "\n",
    "\n",
    "x = np.asarray(X)\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "y = scaler.fit_transform(np.expand_dims(np.array(Y),1))\n",
    "# temp = y>5\n",
    "# x = x[temp]\n",
    "# y = y[temp]\n",
    "# fct = np.max(y)/2\n",
    "# y = y/fct\n",
    "\n",
    "\n",
    "\n",
    "perc_tr = np.random.choice(x.shape[0],95000,replace = False)\n",
    "perc_ts = np.setdiff1d(np.arange(0,x.shape[0]),perc_tr)\n",
    "\n",
    "\n",
    "\n",
    "if scale:\n",
    "    X_n = minmax_scale(x,axis = 1)\n",
    "else:\n",
    "    X_n = x\n",
    "\n",
    "\n",
    "\n",
    "X_tr, Y_tr = X_n[perc_tr], y[perc_tr,:]\n",
    "X_ts, Y_ts = X_n[perc_ts], y[perc_ts,:]\n",
    "\n",
    "\n",
    "\n",
    "X_tr = np.expand_dims(X_tr,1)\n",
    "X_ts = np.expand_dims(X_ts,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Generating Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_CNN(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset,  labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.label = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        data = self.dataset[idx, :].astype('float32')\n",
    "        label1 = self.label[idx, :]\n",
    "        data = torch.from_numpy(data)\n",
    "        label1 = torch.from_numpy(label1.astype('float32'))\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data,label1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class generating Batch Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(X_tr, Y_tr, X_ts, Y_ts, batch_size = 500, valid_size = 0.20):\n",
    "    dataset_train = dataset_CNN(X_tr, Y_tr)\n",
    "    dataset_test_h = dataset_CNN(X_ts, Y_ts)\n",
    "\n",
    "    num_train = len(dataset_train)\n",
    "    num_test_h = len(dataset_test_h)\n",
    "    indices = list(range(num_train))\n",
    "    test_idx_h = list(range(num_test_h))\n",
    "    np.random.shuffle(test_idx_h)\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    test_sampler_h = SubsetRandomSampler(test_idx_h)\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size,\n",
    "        sampler=train_sampler, num_workers=0)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, \n",
    "        sampler=valid_sampler, num_workers=0)\n",
    "    test_loader_h = torch.utils.data.DataLoader(dataset_test_h, batch_size=batch_size, \n",
    "        sampler=test_sampler_h, num_workers=0)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader_h, valid_idx, train_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutioanl Neural Network module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, H, num_conv, num_linear, n_filters, k_size, drop = 0.3):\n",
    "        super(CNN, self).__init__()\n",
    "        self.H = H\n",
    "        self.num_linear = num_linear\n",
    "        self.num_conv = num_conv\n",
    "        self.drop = drop       \n",
    "        self.n_filters = n_filters\n",
    "        self.k_size = k_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, self.n_filters, self.k_size, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=self.n_filters)\n",
    "        self.dropout1 = nn.Dropout(self.drop)\n",
    "        self.size_out = self.size_calc(self.input_size) \n",
    "        self.bn = torch.nn.ModuleList([torch.nn.BatchNorm1d(num_features = self.n_filters) for i in range(self.num_conv)])\n",
    "        self.dropout = torch.nn.ModuleList([torch.nn.Dropout(self.drop) for i in range(self.num_conv)])\n",
    "        self.dropout_lin = torch.nn.ModuleList([torch.nn.Dropout(self.drop) for i in range(self.num_linear)])\n",
    "        self.linears = torch.nn.ModuleList([torch.nn.Linear(self.H, self.H) for i in range(self.num_linear)])\n",
    "        self.convs = torch.nn.ModuleList([torch.nn.Conv1d(self.n_filters, self.n_filters, self.k_size, padding = 1) for i in range(self.num_conv)])\n",
    "        for i in range(len(self.convs)):\n",
    "            self.size_out = self.size_calc(self.size_out)\n",
    "        self.size_out = self.size_out*self.n_filters\n",
    "        self.linear_flat = nn.Linear(self.size_out,self.H)\n",
    "        self.linear_final = nn.Linear(self.H, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        #x = F.relu((self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        for i, l in enumerate(self.convs):\n",
    "            x = F.relu(self.bn[i](self.convs[i](x)))\n",
    "            #x = F.relu((self.convs[i](x)))\n",
    "            x = self.dropout[i](x)\n",
    "        x = x.view(-1, x.shape[2]*self.n_filters)\n",
    "        x = self.linear_flat(x)\n",
    "        for m,n in enumerate(self.linears):\n",
    "            x = F.relu(self.linears[m](x))\n",
    "            x = self.dropout_lin[m](x)\n",
    "        x = F.tanh(self.linear_final(x))\n",
    "        return x\n",
    "    \n",
    "    def size_calc(self, size_in):\n",
    "        size_out = size_in +3 -self.k_size\n",
    "        return size_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_non_CNN(train_loader, valid_loader, input_size,  H=500, ler =0.001, wd = 0, nl = 3, nc = 3, nf = 9, ks = 3, dr = 0.3, n_epochs = 300,  direc= r'C:\\Users\\lbg\\OneDrive - CSEM S.A\\Bureau\\H0_new\\QSO-timedelay'):\n",
    "    model = CNN(input_size, H, nc, nl, nf, ks, drop = dr)\n",
    "    model.float().cuda()\n",
    "    print(model)\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=ler, weight_decay = wd)\n",
    "    valid_loss_min = np.Inf \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "        model.eval()\n",
    "        for data, target in valid_loader:\n",
    "            data = data.cuda()\n",
    "            target=target.cuda()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        train_loss = train_loss/len(train_loader.sampler)\n",
    "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))\n",
    "        \n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), direc)\n",
    "            valid_loss_min = valid_loss\n",
    "    return model\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader_h, valid_idx, train_idx = dataset_loader(X_tr, Y_tr, X_ts, Y_ts, batch_size = 1000, valid_size = 0.20)\n",
    "H_ = [150]\n",
    "lr_ = [0.001]\n",
    "nl_ = [1]\n",
    "convs_ = [5]\n",
    "filters_ = [6]\n",
    "sizes_ = [9]\n",
    "\n",
    "counter = 1\n",
    "for convs in convs_:\n",
    "    for filters in filters_:     \n",
    "        for sizes in sizes_:\n",
    "            for h in H_:\n",
    "                for l in lr_:\n",
    "                    for lay in nl_:\n",
    "                        print('###################### Starting iteration number: ', counter, '######################')\n",
    "                        print()\n",
    "                        name = r'C:\\Users\\lbg\\Desktop\\Pol_{0}_{1}_{2}_{3}_{4}_{5}_100_epochs_bn.pt'.format(str(convs),str(filters),str(sizes),str(h), str(l), str(lay))\n",
    "                        model = train_non_CNN(train_loader, valid_loader, X_tr.shape[2],  H=h, ler =l, wd = 0, nl = lay, nc = convs, nf = filters, ks = sizes, dr = 0.3, n_epochs = 1500, direc= name)\n",
    "                        model.apply(init_weights)\n",
    "                        counter += 1\n",
    "                        print(convs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('Pol_5_6_9_150_0.001_1_100_epochs_bn.pt'))\n",
    "model.eval()\n",
    "loss = 0\n",
    "cnt = 0\n",
    "for i in range(len(X_ts)):\n",
    "    n = i\n",
    "    pred = scaler.inverse_transform(np.expand_dims(model(torch.from_numpy(np.expand_dims(X_ts[n],0).astype('float32')).cuda()).detach().cpu().numpy()[0],1))\n",
    "#     print('Prediction:' ,pred)\n",
    "#     print('Ground_Truth: ', Y_ts[n][0]*fct)\n",
    "    loss += np.power(pred-scaler.inverse_transform(np.expand_dims(Y_ts[n],1)),2)\n",
    "    cnt += 1\n",
    "#     print()\n",
    "    \n",
    "print('Test RMSE: ', np.sqrt(loss/cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test RMSE: ', np.sqrt(loss/cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "loss = 0\n",
    "cnt = 0\n",
    "p = []\n",
    "g = []\n",
    "for i in range(len(X_ts)):\n",
    "    n = i\n",
    "    pred = scaler.inverse_transform(np.expand_dims(model(torch.from_numpy(np.expand_dims(X_ts[n],0).astype('float32')).cuda()).detach().cpu().numpy()[0],1))\n",
    "    #print('Prediction:' ,pred)\n",
    "    p.append(pred)\n",
    "    g.append(scaler.inverse_transform(np.expand_dims(Y_ts[n],1)))\n",
    "    #print('Ground_Truth: ', scaler.inverse_transform(np.expand_dims(Y_ts[n],1)))\n",
    "#    loss += np.power(pred-Y_ts[n][0]*fct,2)\n",
    "#    cnt += 1\n",
    "    print()\n",
    "    \n",
    "#print('Test RMSE: ', np.sqrt(loss/cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for i in range(len(g)):\n",
    "    if np.abs(p[i][0][0]-g[i][0][0])<5:\n",
    "        print(p[i][0][0], g[i][0][0])\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array([p[i][0][0] for i in range(len(p))])\n",
    "gt = np.array([g[i][0][0] for i in range(len(g))])\n",
    "\n",
    "\n",
    "errors = np.abs(pred-gt)\n",
    "maxs = np.max(gt)\n",
    "mins = np.min(gt)\n",
    "\n",
    "mean_errors = stats.binned_statistic(gt,errors, statistic = 'mean')\n",
    "std_errors = stats.binned_statistic(gt,errors, statistic = 'std')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = mean_errors[1]+5\n",
    "new_means = temp[:-1]\n",
    "new_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(new_means,mean_errors[0],std_errors[0])\n",
    "max_sys_m = mean_errors[0][1:].max()\n",
    "max_sys_s = std_errors[0][1:].max()\n",
    "print(max_sys_m)\n",
    "print(max_sys_s)\n",
    "print(np.sqrt(max_sys_s**2+max_sys_m**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pred,gt, s = 0.1)\n",
    "x = np.arange(0,100)\n",
    "y = x\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on real curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hope = np.loadtxt('HE0435_B_sum_D.txt')\n",
    "scaler3 = MinMaxScaler()\n",
    "new = scaler3.fit_transform(np.expand_dims(hope,axis =1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(np.expand_dims(model(torch.from_numpy(np.expand_dims(new,0).astype('float32')).cuda()).detach().cpu().numpy()[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ELEARN",
   "language": "python",
   "name": "elearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
